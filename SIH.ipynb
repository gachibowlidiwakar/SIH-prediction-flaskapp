{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22376,"status":"ok","timestamp":1702298639433,"user":{"displayName":"Sriyans Ketavarapu","userId":"02252141626944290692"},"user_tz":-330},"id":"GYdYkihx5rLO","outputId":"bca92b1a-2850-4993-882d-6b77e97009cf"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":53240,"status":"ok","timestamp":1702299094115,"user":{"displayName":"Sriyans Ketavarapu","userId":"02252141626944290692"},"user_tz":-330},"id":"97n7MK-x5zz1","outputId":"0319ede4-5820-4f25-a969-3404e02c97f2"},"outputs":[{"name":"stdout","output_type":"stream","text":["[0. 1.]\n","Epoch 1/5\n","69/69 [==============================] - 4s 49ms/step - loss: 0.2565 - accuracy: 0.8925 - val_loss: 0.1170 - val_accuracy: 0.9791\n","Epoch 2/5\n","69/69 [==============================] - 3s 45ms/step - loss: 0.0937 - accuracy: 0.9914 - val_loss: 0.1088 - val_accuracy: 0.9882\n","Epoch 3/5\n","69/69 [==============================] - 3s 44ms/step - loss: 0.0766 - accuracy: 0.9975 - val_loss: 0.1064 - val_accuracy: 0.9891\n","Epoch 4/5\n","69/69 [==============================] - 3s 42ms/step - loss: 0.0689 - accuracy: 0.9989 - val_loss: 0.1160 - val_accuracy: 0.9837\n","Epoch 5/5\n","69/69 [==============================] - 3s 44ms/step - loss: 0.0622 - accuracy: 0.9993 - val_loss: 0.1097 - val_accuracy: 0.9873\n"]},{"data":{"text/plain":["<keras.src.callbacks.History at 0x24434231890>"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense\n","from keras.utils import to_categorical\n","\n","# Load the dataset\n","data = pd.read_csv('./SMS.csv')\n","\n","# Map labels to numerical values\n","data['LABEL'] = data['LABEL'].map({'Smishing': 1, 'ham': 0})\n","data.dropna(subset=['LABEL'], inplace=True)\n","data.reset_index(drop=True, inplace=True)\n","# Prepare data for training\n","X = data['TEXT']\n","y = data['LABEL']\n","print(y.unique())\n","\n","# Tokenize text\n","max_words = 10000  # Define the maximum number of words to keep\n","max_length = 200  # Define the sequence length\n","tokenizer = Tokenizer(num_words=max_words)\n","tokenizer.fit_on_texts(X)\n","X_sequences = tokenizer.texts_to_sequences(X)\n","\n","# Pad sequences to fixed length\n","X_padded = pad_sequences(X_sequences, maxlen=max_length)\n","\n","# Convert labels to categorical\n","y_categorical = to_categorical(y)\n","\n","# Split data into train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X_padded, y_categorical, test_size=0.2, random_state=42)\n","\n","# Define the CNN model\n","embedding_dim = 100\n","filters = 128\n","kernel_size = 5\n","\n","model = Sequential()\n","model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_length))\n","model.add(Conv1D(filters=filters, kernel_size=kernel_size, activation='relu'))\n","model.add(MaxPooling1D())\n","model.add(Flatten())\n","model.add(Dense(10, activation='relu'))\n","model.add(Dense(2, activation='softmax'))  # Two classes: 'Smishing' and 'ham'\n","\n","# Compile the model\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","\n","# Train the model\n","model.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_test, y_test))\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'keras.utils.vis_utils'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvis_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m model_to_dot\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SVG\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Assuming 'model' is already defined as per your script\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Convert the model to dot format\u001b[39;00m\n","\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras.utils.vis_utils'"]}],"source":[]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2185,"status":"ok","timestamp":1702301256590,"user":{"displayName":"Sriyans Ketavarapu","userId":"02252141626944290692"},"user_tz":-330},"id":"pZ605Fp_6JO6","outputId":"7c7d7753-87c1-435b-bfa4-5d91ebf5f897"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\daksh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 0s 62ms/step\n","[[0.39506707 0.60493296]]\n","1\n","The model classifies the message as: 'Smishing'\n"]}],"source":["import numpy as np\n","import pickle\n","#Save the trained model\n","model.save('./sms_model5.h5')\n","# Save the model using pickle\n","with open('sms_model_pickle.pkl', 'wb') as file:\n","    pickle.dump(model, file)\n","\n","# Load the saved model\n","from keras.models import load_model\n","loaded_model = load_model('./sms_model5.h5')\n","\n","# Custom message for classification\n","new_message = \"collect your lottery of 2500000 here immediately. Click on this link\"\n","\n","# Tokenize and pad the new message\n","new_message_sequence = tokenizer.texts_to_sequences([new_message])\n","new_message_padded = pad_sequences(new_message_sequence, maxlen=max_length)\n","\n","# Classify the new message\n","prediction = loaded_model.predict(new_message_padded)\n","print(prediction)\n","predicted_label = np.argmax(prediction)\n","print(predicted_label)\n","# Decode the predicted label\n","label_mapping = {0: 'ham', 1: 'Smishing'}\n","predicted_class = label_mapping[predicted_label]\n","\n","print(f\"The model classifies the message as: '{predicted_class}'\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HU_ojiM3C11N"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"z0CzajvF6iIy"},"source":["# New section"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNqa5TPFtNKLgLZ6ZiyhAzN","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":0}
